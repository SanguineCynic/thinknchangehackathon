<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TREADR</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="nav.css">
</head>
<body>
    <header id="header">
        <div id="treadr">
            <span>T</span>
            <span>R</span>
            <span>E</span>
            <span>A</span>
            <span>D</span>
            <span>R</span>
           </div>
           
       </header>
       <nav id="navbar">
        <ul>
         <li><a href="index.html">Home</a></li>
         <li><a href="facetrain.html" class="currentPage">Face Training</a></li>
         <li><a href="contact.html">Contact Us</a></li>
         <li><a href="seekhelp.html">Seek Help</a></li>
        </ul>
       </nav>
<div id="infoBox">
<h1> Facial Recognition?</h1>
<hr>
    <p>
        "Participants with autism had a significantly lower accuracy in recognizing facial expressions compared to typically developing participants. This suggests that autistic people may indeed have difficulty with facial expressions.<br><br>
        The study also found that the difficulty was not related to the age or IQ of the participants. This indicates that the issue is not about the cognitive abilities of the autistic individuals, but rather about their specific difficulty in interpreting facial expressions."<br><br>
        <em>-Hannah Meyer-Lindenberg et al. Mol Autism. 2022; 13: 43</em> <br><br>

        The Indentify Practice tool provided below is designed for autistic persons to practice recognizing emotions through facial expressions with the guidance of an adult. It offers a range of randomly generated faces displaying different emotions, providing a helpful platform for interactive learning and skill development.<br><br>

        The Reflection Practice tool utilizes AI to evaluate and judge facial expressions, allowing individuals to gauge how recognizable their own expressions are. By capturing and analyzing facial cues, this AI-powered tool provides feedback on the clarity and identifiability of expressions, aiding users in refining their ability to convey emotions effectively.
    </p>
</div>

<div id="identifyPracticeBanner">
    <h1>Identify Practice</h1>
   </div>

   

    <img id="dynamicImage" src="https://thispersondoesnotexist.com/" width="50%" height="50%">
    <button id="refreshButton">New Image</button>

    <div id="identifyPracticeBanner">
        <h1>Reflection Practice</h1>
       </div>

       <!-- <div>Teachable Machine Image Model</div> -->
       <center><div id="webcam-container"></div>
       <div id="label-container"></div></center>
       <button type="button" onclick="init()" id="startButton">Start</button>
       <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>
       <script src="https://cdn.jsdelivr.net/npm/@teachablemachine/image@latest/dist/teachablemachine-image.min.js"></script>
       <script type="text/javascript">
           // More API functions here:
           // https://github.com/googlecreativelab/teachablemachine-community/tree/master/libraries/image
       
           // the link to your model provided by Teachable Machine export panel
           const URL = "https://teachablemachine.withgoogle.com/models/_hBXL15Zk/";
       
           let model, webcam, labelContainer, maxPredictions;
       
           // Load the image model and setup the webcam
           async function init() {
               const modelURL = URL + "model.json";
               const metadataURL = URL + "metadata.json";
       
               // load the model and metadata
               // Refer to tmImage.loadFromFiles() in the API to support files from a file picker
               // or files from your local hard drive
               // Note: the pose library adds "tmImage" object to your window (window.tmImage)
               model = await tmImage.load(modelURL, metadataURL);
               maxPredictions = model.getTotalClasses();
       
               // Convenience function to setup a webcam
               const flip = true; // whether to flip the webcam
               webcam = new tmImage.Webcam(200, 200, flip); // width, height, flip
               await webcam.setup(); // request access to the webcam
               await webcam.play();
               window.requestAnimationFrame(loop);
       
               // append elements to the DOM
               document.getElementById("webcam-container").appendChild(webcam.canvas);
               labelContainer = document.getElementById("label-container");
               for (let i = 0; i < maxPredictions; i++) { // and class labels
                   labelContainer.appendChild(document.createElement("div"));
               }
           }
       
           async function loop() {
               webcam.update(); // update the webcam frame
               await predict();
               window.requestAnimationFrame(loop);
           }
       
           // run the webcam image through the image model
           async function predict() {
               // predict can take in an image, video or canvas html element
               const prediction = await model.predict(webcam.canvas);
               for (let i = 0; i < maxPredictions; i++) {
                   const classPrediction =
                       prediction[i].className + ": " + prediction[i].probability.toFixed(2);
                   labelContainer.childNodes[i].innerHTML = classPrediction;
               }
           }
       </script>
       

<script src="script.js"></script>

</body>
</html>